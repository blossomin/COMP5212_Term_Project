{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n",
    "#%pip install efficientnet_pytorch\n",
    "#%pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "urbjiiokl430h0wt13n8j"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from efficientnet_pytorch import model as enet\n",
    "import albumentations as albu\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning) \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "77k9rr39flsmzx7vfecyv"
   },
   "outputs": [],
   "source": [
    "VER = 'v4'\n",
    "DEBUG = False\n",
    "PARAMS = {\n",
    "    'version': VER,\n",
    "    'folds': 5,\n",
    "    'folds_train': None,\n",
    "    'img_size': 300, #224=B0 240=B1 260=B2 300=B3 380=B4 456=B5\n",
    "    'resize': 4,\n",
    "    'batch_size': 20,\n",
    "    'workers': 20,\n",
    "    'epochs': 10 if DEBUG else 40,\n",
    "    'warmup': False,\n",
    "    'dropout': .4,\n",
    "    'backbone': 'efficientnet-b3',\n",
    "    'seed': 2020,\n",
    "    'aughard': True,\n",
    "    'lr': .001,\n",
    "    'average': 'samples', # 'micro', 'macro' or 'samples'\n",
    "    'comments': 'f1 score'\n",
    "}\n",
    "DATA_PATH = './data'\n",
    "IMGS_PATH = f'{DATA_PATH}/train_images/'\n",
    "MDLS_PATH = f'./models_{VER}'\n",
    "if not os.path.exists(MDLS_PATH):\n",
    "    os.mkdir(MDLS_PATH)\n",
    "    \n",
    "def seed_all(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_all(PARAMS['seed'])\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "dr6m6fxy9ow1h7o7tcjpxe"
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    df_train = pd.read_csv(f'{DATA_PATH}/train.csv').sample(100).reset_index(drop=True)\n",
    "else:\n",
    "    df_train = pd.read_csv(f'{DATA_PATH}/train.csv')\n",
    "df_sub = pd.read_csv(f'{DATA_PATH}/sample_submission.csv')\n",
    "display(df_train.head())\n",
    "display(df_train.labels.value_counts())\n",
    "labels = []\n",
    "for lbl in list(set(df_train.labels)):\n",
    "    labels.extend(lbl.split())\n",
    "labels = list(set(labels))\n",
    "LABELS = {i: x for i, x in enumerate(sorted(labels))}\n",
    "LABELS_ = {x: i for i, x in enumerate(sorted(labels))}\n",
    "PARAMS['labels'] = LABELS\n",
    "PARAMS['labels_'] = LABELS_\n",
    "with open(f'{MDLS_PATH}/params.json', 'w') as file:\n",
    "    json.dump(PARAMS, file)\n",
    "print('labels:', LABELS)\n",
    "print('labels_:', LABELS_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ud7o16tr9pivu93sxwc1j"
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(PARAMS['folds'], shuffle=True, random_state=PARAMS['seed'])\n",
    "df_train['fold'] = -1\n",
    "for i, (train_idx, valid_idx) in enumerate(skf.split(df_train, df_train['labels'])):\n",
    "    df_train.loc[valid_idx, 'fold'] = i\n",
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_occurence = pd.DataFrame({\n",
    "    'origin': df_train.labels.value_counts(normalize=True),\n",
    "    'fold_0': df_train[df_train.fold == 0].labels.value_counts(normalize=True),\n",
    "    'fold_1': df_train[df_train.fold == 1].labels.value_counts(normalize=True),\n",
    "    'fold_2': df_train[df_train.fold == 2].labels.value_counts(normalize=True),\n",
    "    'fold_3': df_train[df_train.fold == 3].labels.value_counts(normalize=True),\n",
    "    'fold_4': df_train[df_train.fold == 4].labels.value_counts(normalize=True)})\n",
    "df_occurence.plot.barh(figsize=[12, 6], colormap='plasma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "if PARAMS['aughard']:\n",
    "    aug = albu.Compose([\n",
    "        albu.OneOf([\n",
    "            albu.RandomBrightness(limit=.2, p=1), \n",
    "            albu.RandomContrast(limit=.2, p=1), \n",
    "            albu.RandomGamma(p=1)\n",
    "        ], p=.5),\n",
    "        albu.OneOf([\n",
    "            albu.Blur(blur_limit=3, p=1),\n",
    "            albu.MedianBlur(blur_limit=3, p=1)\n",
    "        ], p=.25),\n",
    "        albu.OneOf([\n",
    "            albu.GaussNoise(0.002, p=.5),\n",
    "            albu.IAAAffine(p=.5),\n",
    "        ], p=.25),\n",
    "        albu.RandomRotate90(p=.5),\n",
    "        albu.HorizontalFlip(p=.5),\n",
    "        albu.VerticalFlip(p=.5),\n",
    "        albu.Transpose(p=.5),\n",
    "        albu.Cutout(\n",
    "            num_holes=10, \n",
    "            max_h_size=int(.1 * PARAMS['img_size']), \n",
    "            max_w_size=int(.1 * PARAMS['img_size']), \n",
    "            p=.25),\n",
    "        albu.ShiftScaleRotate(p=.5)\n",
    "    ])\n",
    "else:\n",
    "    aug = albu.Compose([\n",
    "        albu.OneOf([\n",
    "            albu.RandomBrightness(limit=.2, p=1), \n",
    "            albu.RandomContrast(limit=.2, p=1), \n",
    "            albu.RandomGamma(p=1)\n",
    "        ], p=.5),\n",
    "        albu.RandomRotate90(p=.25),\n",
    "        albu.HorizontalFlip(p=.25),\n",
    "        albu.VerticalFlip(p=.25)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "lbk4zrrpxoh7w0i0fmye"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "def flip(img, axis=0):\n",
    "    if axis == 1:\n",
    "        return img[::-1, :, ]\n",
    "    elif axis == 2:\n",
    "        return img[:, ::-1, ]\n",
    "    elif axis == 3:\n",
    "        return img[::-1, ::-1, ]\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "class PlantDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, df, size, labels, transform=None, tta=0):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.size = size\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.tta = tta\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        img_name = row.image\n",
    "        img_path = f'{IMGS_PATH}/{img_name}'\n",
    "        img = cv2.imread(img_path)\n",
    "        if not np.any(img):\n",
    "            print('no img file read:', img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (self.size, self.size))\n",
    "        img = img.astype(np.float32) / 255\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(image=img)['image']\n",
    "        if self.labels:\n",
    "            img = img.transpose(2, 0, 1)\n",
    "            label = np.zeros(len(self.labels)).astype(np.float32)\n",
    "            for lbl in row.labels.split():\n",
    "                label[self.labels[lbl]] = 1\n",
    "            return torch.tensor(img), torch.tensor(label)\n",
    "        else:\n",
    "            img = flip(img, axis=self.tta)\n",
    "            img = img.transpose(2, 0, 1)\n",
    "            return torch.tensor(img.copy())\n",
    "\n",
    "dataset_show = PlantDataset(\n",
    "    df=df_train,\n",
    "    size=PARAMS['img_size'],\n",
    "    labels=LABELS_,\n",
    "    transform=aug\n",
    ")\n",
    "img_test, lbl_test = dataset_show.__getitem__(7)\n",
    "img_test = img_test.numpy().transpose([1, 2, 0])\n",
    "img_test = np.clip(img_test, 0, 1)\n",
    "plt.imshow(img_test)\n",
    "plt.title(lbl_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "4kzltijd320tujsid3q11h"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train_epoch(loader, optimizer):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    bar = tqdm(loader, desc='ep')\n",
    "    for (data, target) in bar:\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        loss_func = criterion\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data)\n",
    "        loss = loss_func(logits, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_np = loss.detach().cpu().numpy()\n",
    "        train_loss.append(loss_np)\n",
    "        smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n",
    "        bar.set_description('loss: {:.4f}, smth: {:.4f}'.format(loss_np, smooth_loss))\n",
    "    return train_loss\n",
    "\n",
    "def val_epoch(loader, get_output=False, verbose=False):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    val_logits = []\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for (data, target) in tqdm(loader):\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            logits = model(data)\n",
    "            loss = criterion(logits, target)\n",
    "            pred = logits.sigmoid().detach().round()\n",
    "            val_logits.append(logits)\n",
    "            val_preds.append(pred)\n",
    "            val_targets.append(target)\n",
    "            val_loss.append(loss.detach().cpu().numpy())\n",
    "        val_loss = np.mean(val_loss)\n",
    "    val_logits = torch.cat(val_logits).cpu().numpy()\n",
    "    val_preds = torch.cat(val_preds).cpu().numpy()\n",
    "    val_targets = torch.cat(val_targets).cpu().numpy()\n",
    "    val_acc = (val_preds == val_targets).mean() * 100\n",
    "    val_f1 = f1_score(val_targets, val_preds, average=PARAMS['average'])\n",
    "    if verbose:\n",
    "        print('val acc: {:.2f} | val loss: {:.4f} | val f1: {:.4f}'.format(val_acc, val_loss, val_f1))\n",
    "    if get_output:\n",
    "        return val_logits\n",
    "    else:\n",
    "        return val_loss, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "wp3xdq2ddaascfg12mf2p"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "class EffNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, params, out_dim):\n",
    "        super(EffNet, self).__init__()\n",
    "        self.enet = enet.EfficientNet.from_name(params['backbone'])\n",
    "        nc = self.enet._fc.in_features\n",
    "        self.enet._fc = nn.Identity()\n",
    "        self.myfc = nn.Sequential(\n",
    "            nn.Dropout(params['dropout']),\n",
    "            nn.Linear(nc, int(nc / 4)),\n",
    "            nn.Dropout(params['dropout']),\n",
    "            nn.Linear(int(nc / 4), out_dim)\n",
    "        )\n",
    "        \n",
    "    def extract(self, x):\n",
    "        return self.enet(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.extract(x)\n",
    "        x = self.myfc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "fikuv467ij60s6i4qvfo1lm",
    "execution_id": "14279c6b-7c6e-4f04-984a-13d3f2d177cd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "pred, target = [], []\n",
    "\n",
    "if DEBUG:\n",
    "    n_folds_train = 2\n",
    "else:\n",
    "    n_folds_train = PARAMS['folds'] if not PARAMS['folds_train'] else PARAMS['folds_train']\n",
    "start_folds_train = 0\n",
    "\n",
    "for fold_num in range(start_folds_train, n_folds_train):\n",
    "    print('=' * 20, 'FOLD:', fold_num, '=' * 20)\n",
    "    train_idxs = np.where((df_train['fold'] != fold_num))[0]\n",
    "    val_idxs = np.where((df_train['fold'] == fold_num))[0]\n",
    "    df_fold  = df_train.loc[train_idxs]\n",
    "    df_val = df_train.loc[val_idxs]\n",
    "    dataset_train = PlantDataset(\n",
    "        df=df_fold,\n",
    "        size=PARAMS['img_size'],\n",
    "        labels=LABELS_,\n",
    "        transform=aug\n",
    "    )\n",
    "    dataset_val = PlantDataset(\n",
    "        df=df_val,\n",
    "        size=PARAMS['img_size'],\n",
    "        labels=LABELS_,\n",
    "        transform=None\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train, \n",
    "        batch_size=PARAMS['batch_size'], \n",
    "        sampler=RandomSampler(dataset_train), \n",
    "        num_workers=PARAMS['workers']\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        dataset_val, \n",
    "        batch_size=PARAMS['batch_size'], \n",
    "        sampler=SequentialSampler(dataset_val), \n",
    "        num_workers=PARAMS['workers']\n",
    "    )\n",
    "    model = EffNet(params=PARAMS, out_dim=len(LABELS_)) \n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=PARAMS['lr'])\n",
    "    if PARAMS['warmup']:\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, \n",
    "            max_lr=PARAMS['lr'], \n",
    "            total_steps=PARAMS['epochs'],\n",
    "            div_factor=(PARAMS['lr'] / 1e-5), \n",
    "            final_div_factor=1000,\n",
    "            pct_start=(int(.1 * PARAMS['epochs']) / PARAMS['epochs']),\n",
    "        )\n",
    "    else:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, PARAMS['epochs'])\n",
    "    print('train len:', len(dataset_train),'| val len:', len(dataset_val))\n",
    "    best_file = '{}/model_best_{}.pth'.format(MDLS_PATH, fold_num)\n",
    "    acc_max = 0\n",
    "    f1_max = 0\n",
    "    for epoch in tqdm(range(PARAMS['epochs']), desc='epochs'):\n",
    "        print(time.ctime(), 'epoch:', epoch)\n",
    "        train_loss = train_epoch(train_loader, optimizer)\n",
    "        val_loss, acc, f1 = val_epoch(val_loader)\n",
    "        scheduler.step(epoch)\n",
    "        content = '{} epoch {}, lr: {:.8f}, train loss: {:.4f}, val loss: {:.4f}, acc: {:.2f}, val f1: {:.4f}'.format(\n",
    "            time.ctime(),\n",
    "            epoch, \n",
    "            optimizer.param_groups[0]['lr'], \n",
    "            np.mean(train_loss),\n",
    "            np.mean(val_loss),\n",
    "            acc,\n",
    "            f1\n",
    "        )\n",
    "        print(content)\n",
    "        with open('{}/log_{}.txt'.format(MDLS_PATH, fold_num), 'a') as appender:\n",
    "            appender.write(content + '\\n')\n",
    "        if f1 > f1_max:\n",
    "            torch.save(model.state_dict(), best_file)\n",
    "            print('f1 improved {:.2f} --> {:.2f} model saved'.format(f1_max, f1))\n",
    "            f1_max = f1\n",
    "    with open('log_total.txt', 'a') as appender:\n",
    "        appender.write('{} | fold: {} | max f1: {:.2f}\\n'.format(PARAMS, fold_num, f1_max))\n",
    "    torch.save(\n",
    "        model.state_dict(), \n",
    "        os.path.join('{}/model_final_{}.pth'.format(MDLS_PATH, fold_num))\n",
    "    )\n",
    "    del model, dataset_train, dataset_val, train_loader, val_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orange Python 3",
   "language": "python",
   "name": "orange"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "notebookId": "e589dbea-535a-4741-baa8-090d3c1565e1"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
